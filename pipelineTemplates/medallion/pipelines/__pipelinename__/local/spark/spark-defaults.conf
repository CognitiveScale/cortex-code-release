spark.app.name={{pipelinename}} - Sensa Data Pipeline
spark.master=local[*]
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.ui.enabled=true
spark.cortex.client.secrets.json={'test-project':{'s3-secret':'<REPLACE_ME>'}}
spark.cortex.client.secrets.impl=com.c12e.cortex.profiles.client.LocalSecretClient
spark.cortex.catalog.impl=com.c12e.cortex.phoenix.LocalCatalog
spark.cortex.catalog.local.dir=pipelines/{{pipelinename}}/local/specs
spark.cortex.client.storage.impl=com.c12e.cortex.profiles.client.LocalRemoteStorageClient
spark.cortex.storage.storageType=file
spark.cortex.storage.file.baseDir=/tmp/local-pipelines/{{pipelinename}}/local-data
spark.cortex.storage.bucket.profiles=cortex-profiles
spark.cortex.storage.bucket.content=cortex-content
spark.cortex.storage.bucket.amp=cortex-amp
spark.sql.parquet.datetimeRebaseModeInWrite=CORRECTED
spark.jars.ivySettings=ivy.settings
spark.sql.catalogImplementation=in-memory
spark.databricks.delta.properties.defaults.enableChangeDataFeed=true
# TODO: How to automate the usage of this file
spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.4,com.c12e.cortex.profiles:profiles-sdk:1.8.14-g16ce6b4,org.mongodb.spark:mongo-spark-connector_2.12:3.0.2,org.postgresql:postgresql:42.6.0
